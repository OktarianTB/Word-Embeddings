{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# Implementation of the Word2Vec Algorithm 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources used: [Intuitive Guide to Understanding Word2Vec - by Thushan Ganegedara](https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-understanding-word2vec-e0128a460f0f)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from matplotlib import pylab\n",
    "from sklearn.manifold import TSNE\n",
    "import operator\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have scraped the wikipedia page for 'France' and stored the text in a file called france.txt for use in this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Data size 26458\nExample words (start):  ['republic', 'in', 'europe', 'with', 'several', 'non', 'european', 'regions', 'french', 'republic']\nExample words (end):  ['won', 'two', 'olympic', 'silver', 'medals', 'in', '2000', 'and', '1948', '']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Clean the data to make it usable later\"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace('.',' ')\n",
    "    text = text.replace(',',' ')\n",
    "    text = text.replace('-',' ')\n",
    "    text = text.replace('%','')\n",
    "    text = text.replace(':',' ')\n",
    "    text = text.replace('(','')\n",
    "    text = text.replace(')','')\n",
    "    text = text.replace('\\n',' ')\n",
    "    text = text.replace('[',' ')\n",
    "    text = text.replace(']',' ')\n",
    "    text = text.replace('\"',' ')\n",
    "    text = text.replace('  ',' ')\n",
    "    text = text.replace('   ',' ')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    return text\n",
    "\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "\n",
    "    with open(filename,'r') as f:\n",
    "        data = []\n",
    "        file_string = f.read()\n",
    "        file_string = preprocess_text(file_string)\n",
    "        file_string = file_string.split(' ')\n",
    "        data.extend(file_string)\n",
    "    return data\n",
    "  \n",
    "words = read_data('france.txt')\n",
    "print('Data size %d' % len(words))\n",
    "print('Example words (start): ',words[:10])\n",
    "print('Example words (end): ',words[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Most common words (+UNK) [['UNK', 10245], ('', 2022), ('the', 1996), ('of', 1005), ('and', 860), ('in', 732), ('france', 394), ('french', 346), ('to', 329), ('a', 306), ('as', 238), ('is', 233), ('by', 165), ('was', 151), ('with', 132)]\nSample data [47, 4, 38, 13, 92, 0, 34, 126, 6, 47, 0, 0, 77, 6, 0]\nVocabulary size: 327\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "vocabulary_size = 0\n",
    "def build_dataset(words):\n",
    "    global vocabulary_size\n",
    "    \n",
    "    # 'UNK' indicates an unknown word and can then be used as a token to show a missing word\n",
    "    count = [['UNK', -1]]\n",
    "    \n",
    "    # Gets words sorted by frequency\n",
    "    count.extend(collections.Counter(words).most_common())\n",
    "    dictionary = dict()\n",
    "\n",
    "    # For all words that appear >= 10 times in the dataset: Create an ID for each unique word and store it in the dictionary\n",
    "    for word, c in count:\n",
    "        if c<10:\n",
    "            continue\n",
    "        dictionary[word] = len(dictionary)\n",
    "        vocabulary_size += 1\n",
    "\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    \n",
    "    # Words that appear less than 10 times get classified as unknown, the others are classified based on their assigned ID\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  \n",
    "            unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    \n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print('Most common words (+UNK)', count[:15])\n",
    "print('Sample data', data[:15])\n",
    "print('Vocabulary size:', vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/data_illustration.png\" alt=\"Data\" width=\"450\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "data: ['republic', 'in', 'europe', 'with', 'several', '', 'european', 'regions']\n\nwith window_size = 2:\n    batch: ['europe', 'europe', 'europe', 'europe', 'with', 'with', 'with', 'with']\n    labels: ['republic', 'in', 'with', 'several', 'in', 'europe', 'several', '']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, window_size):\n",
    "    global data_index \n",
    "    \n",
    "    # two numpy arras to hold target words (batch)\n",
    "    # and context words (labels)\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    \n",
    "    # span defines the total window size\n",
    "    span = 2 * window_size + 1 \n",
    "\n",
    "    # The buffer holds the data contained within the span\n",
    "    queue = collections.deque(maxlen=span)\n",
    "    # Fill the buffer and update the data_index\n",
    "    for _ in range(span):\n",
    "        queue.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "    for i in range(batch_size // (2*window_size)):\n",
    "        k=0\n",
    "        # Avoid the target word itself as a prediction\n",
    "        for j in list(range(window_size))+list(range(window_size+1,2*window_size+1)):\n",
    "            batch[i * (2*window_size) + k] = queue[window_size]\n",
    "            labels[i * (2*window_size) + k, 0] = queue[j]\n",
    "            k += 1 \n",
    "    \n",
    "        # Everytime we read num_samples data points, update the queue\n",
    "        queue.append(data[data_index])\n",
    "\n",
    "        # If end is reached, circle back to the beginning\n",
    "        data_index = (data_index + np.random.randint(window_size)) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "\n",
    "print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
    "\n",
    "data_index = 0\n",
    "batch, labels = generate_batch(batch_size=8, window_size=2)\n",
    "print('\\nwith window_size = %d:' %2)\n",
    "print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128  # Number of data points in we process at a given time\n",
    "embedding_size = 64  # Size of a word vector\n",
    "window_size = 4  # Size of the context window\n",
    "num_sampled = 32  # Number of negative samples in the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TensorFlow Placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A placeholder is simply a variable that we will assign data to at a later date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Training input data (target word IDs). \n",
    "train_dataset = tf.placeholder(tf.int32, shape=[batch_size]) \n",
    "\n",
    "# Training input label data (context word IDs) \n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Embedding layer\n",
    "# tf.random_uniform: Outputs random values from a uniform distribution\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "# Neural network weights and biases\n",
    "softmax_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                        stddev=0.1 / math.sqrt(embedding_size)))\n",
    "\n",
    "softmax_biases = tf.Variable(tf.random_uniform([vocabulary_size],-0.01,0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Connecting the embedding layer to the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding layer **stores the word vectors of all the words found in the vocabulary**. As you can imaging this is an enormous matrix (of size [vocabulary size x embedding size]). \n",
    "\n",
    "This embedding size is a user-tunable parameter. The higher it is, the better performing your model will be. But weâ€™ll not get much of a jaw-dropping performance/size gain, beyond a certain point (say, an embedding size of 500). This gigantic matrix is initialized randomly (just like a neural network) and is tweaked bit by bit, during the optimization process, to reveal the powerful word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/embedding.png\" alt=\"Embedding\" width=\"450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training, the neural network takes an **input word and attempt to predict the output word**. Then using a loss function, we **penalize the model for incorrect classifications and reward the model for correct classifications**. \n",
    "\n",
    "    1) For a given input word (the target word), find the corresponding word vector from the embedding layer\n",
    "    2) Feed the word vector to the neural network, then try to predict the correct output word (a context word)\n",
    "    3) By comparing the prediction and true context word, compute the loss\n",
    "    4) Use the loss along with a stochastic optimizer to optimize the neural network and the embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'tf.nn.embedding_lookup' function takes our embedding layer as the input and a set of word IDs ('train_dataset') and outputs the corresponding word vectors to the variable 'embed'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Look up embeddings for a batch of inputs. \n",
    "embed = tf.nn.embedding_lookup(embeddings, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sampled Softmax loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you compute the cross entropy loss between the true context word ID for a given target word and the prediction value corresponding to the true context word ID. Then to that, we add the cross entropy loss of K negative samples we sampled according to some noise distribution. On a high level, we define the loss as follows:\n",
    "<img src=\"images/loss_function.png\" alt=\"Embedding\" width=\"600\"/>\n",
    "\n",
    "The takeaway message is that, the sampled softmax loss computes the loss by considering two types of entities:\n",
    "\n",
    "    1) The index given by the true context word ID in the prediction vector (words within the context window)\n",
    "    2) K indices that indicate word IDs, and are considered to be noise (words outside the context window)\n",
    "    \n",
    "<img src=\"images/loss_fc_vis.png\" alt=\"Embedding\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(\n",
    "    weights=softmax_weights, biases=softmax_biases, inputs=embed, \n",
    "    labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize the loss with respect to the parameters of the embeddings layer and the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the normalized embedding layer by making the vector magnitude equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True)) \n",
    "normalized_embeddings = embeddings / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following image recaps what the model does.\n",
    "<img src=\"images/overview.png\" alt=\"Embedding\" width=\"450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Skip-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is a Skip-Gram Model. The Skip-gram model architecture usually tries to achieve the reverse of what the CBOW model does. It tries to predict the source context words (surrounding words) given a target word (the center word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Initialized\n",
      "Average loss at step 5000: 2.623234\n",
      "Average loss at step 10000: 2.381607\n",
      "Average loss at step 15000: 2.353815\n",
      "Average loss at step 20000: 2.338758\n",
      "Average loss at step 25000: 2.334405\n",
      "Average loss at step 30000: 2.321962\n",
      "Average loss at step 35000: 2.310366\n",
      "Average loss at step 40000: 2.311999\n",
      "Average loss at step 45000: 2.306581\n",
      "Average loss at step 50000: 2.308098\n",
      "Average loss at step 55000: 2.302817\n",
      "Average loss at step 60000: 2.314145\n",
      "Average loss at step 65000: 2.301404\n",
      "Average loss at step 70000: 2.297190\n",
      "Average loss at step 75000: 2.292085\n",
      "Average loss at step 80000: 2.302375\n",
      "Average loss at step 85000: 2.294170\n",
      "Average loss at step 90000: 2.297591\n",
      "Average loss at step 95000: 2.298612\n",
      "Average loss at step 100000: 2.297581\n",
      "Average loss at step 105000: 2.290814\n",
      "Average loss at step 110000: 2.285490\n",
      "Average loss at step 115000: 2.290061\n",
      "Average loss at step 120000: 2.291997\n",
      "Average loss at step 125000: 2.288710\n",
      "Average loss at step 130000: 2.279675\n",
      "Average loss at step 135000: 2.280752\n",
      "Average loss at step 140000: 2.283106\n",
      "Average loss at step 145000: 2.288809\n",
      "Average loss at step 150000: 2.289595\n",
      "Average loss at step 155000: 2.292286\n",
      "Average loss at step 160000: 2.288454\n",
      "Average loss at step 165000: 2.293123\n",
      "Average loss at step 170000: 2.288287\n",
      "Average loss at step 175000: 2.278452\n",
      "Average loss at step 180000: 2.281116\n",
      "Average loss at step 185000: 2.288987\n",
      "Average loss at step 190000: 2.286955\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-8ed8a2e862e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# compute loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtrain_dataset\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# Update the average loss variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1354\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1431\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ],
   "source": [
    "num_steps = 250001 \n",
    "session = tf.InteractiveSession() \n",
    "# Initialize the variables in the graph\n",
    "tf.global_variables_initializer().run() \n",
    "print('Initialized') \n",
    "average_loss = 0\n",
    "\n",
    "for step in range(num_steps): \n",
    "    # Generate a single batch of data\n",
    "    batch_data, batch_labels = generate_batch( batch_size, window_size)\n",
    "    \n",
    "    # Optimize the embedding layer and neural network\n",
    "    # compute loss\n",
    "    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "\n",
    "    # Update the average loss variable\n",
    "    average_loss += l\n",
    "\n",
    "    if (step+1) % 5000 == 0:\n",
    "        if step > 0:\n",
    "            average_loss = average_loss / 5000\n",
    "\n",
    "        print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "        average_loss = 0\n",
    "\n",
    "sg_embeddings = normalized_embeddings.eval()\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualize the word vectors by means of the t-SNE dimensionality reduction technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(perplexity=5, n_components=2, init='pca', n_iter=5000)\n",
    "\n",
    "# get the T-SNE manifold\n",
    "two_d_embeddings = tsne.fit_transform(sg_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "selected_words = ['france','republic','europe','the','region','and','french','army','king','empire','tourism']\n",
    "\n",
    "words = [reverse_dictionary[i] for i in np.arange(vocabulary_size)]\n",
    "\n",
    "pylab.figure(figsize=(15,15))\n",
    "# plot all the embeddings and their corresponding words\n",
    "for i, label in enumerate(words):\n",
    "    x, y = two_d_embeddings[i,:]\n",
    "    pylab.scatter(x, y, c='darkgray')   \n",
    "    if label in selected_words:\n",
    "        pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',\n",
    "                       ha='right', va='bottom',fontsize=10)\n",
    "        \n",
    "pylab.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}